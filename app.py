import streamlit as st
st.set_page_config(page_title="ğŸ“š ì†Œì„¤ ìºë¦­í„° ì±—ë´‡", layout="centered")

try:
    import pysqlite3  # type: ignore
    import sys
    sys.modules["sqlite3"] = sys.modules.pop("pysqlite3")
except Exception:
    pass
    
# RAG ê²€ìƒ‰ + í˜ë¥´ì†Œë‚˜ ì£¼ì… + ë‹µë³€ ìƒì„±
from dotenv import load_dotenv
load_dotenv()

import os, re
import chromadb
from rank_bm25 import BM25Okapi
from openai import OpenAI

# ================= ê¸°ë³¸ ì„¤ì • =================
BASE_DIR      = os.path.dirname(os.path.abspath(__file__))
PERSIST_DIR   = os.getenv("PERSIST_DIR") or os.path.join(BASE_DIR, "rag", ".chroma")
COLLECTION    = os.getenv("COLLECTION", "library-all")
MODEL         = os.getenv("MODEL", "gpt-4o")
TOP_K         = int(os.getenv("TOP_K", "6"))
EMB_MODEL     = "text-embedding-3-small"

# OpenAI
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
oa = OpenAI()

WORK_ID_MAP = {
    "ì§€êµ¬ ëì˜ ì˜¨ì‹¤": "jigu-ggut-onshil",
    "ì¢…ì˜ ê¸°ì›": "jong-ui-giwon",
    "ì†Œë…„ì´ ì˜¨ë‹¤": "so-nyeon-i-onda"
}

# ================= ìœ í‹¸/í† í°í™” =================
def tokenize(text: str):
    # ì˜ë¬¸/ìˆ«ì/í•œê¸€ë§Œ ì¶”ì¶œ â†’ ì†Œë¬¸ì â†’ í† í° ë¦¬ìŠ¤íŠ¸
    return re.findall(r"[0-9A-Za-zê°€-í£]+", (text or "").lower())

def reciprocal_rank_fusion(results_lists, k=60):
    scores = {}
    for res in results_lists:
        for rank, doc_id in enumerate(res, start=1):
            scores[doc_id] = scores.get(doc_id, 0) + 1.0/(k+rank)
    return sorted(scores.items(), key=lambda x: x[1], reverse=True)

# ================= Chroma ë¡œë“œ =================
client = chromadb.PersistentClient(path=PERSIST_DIR)
col = client.get_or_create_collection(name=COLLECTION, embedding_function=None)

results = col.get(include=["documents","metadatas"], limit=999_999)
all_ids   = results.get("ids", []) or []
all_docs  = results.get("documents", []) or []
all_metas = results.get("metadatas", []) or []

st.sidebar.write("loaded_from_chroma:", len(all_docs))

# ì•ˆì „ í•„í„°ë§ + í† í°í™” (bm25 ZeroDivisionError ë°©ì§€)
filtered_ids, filtered_docs, filtered_metas, tokenized_docs = [], [], [], []
for doc_id, doc, meta in zip(all_ids, all_docs, all_metas):
    if not isinstance(doc, str):
        continue
    if not doc.strip():
        continue
    toks = tokenize(doc)
    if not toks:
        continue
    filtered_ids.append(doc_id)
    filtered_docs.append(doc)
    filtered_metas.append(meta or {})
    tokenized_docs.append(toks)

st.sidebar.write("after_filter:", len(filtered_docs))

bm25 = None
if tokenized_docs:
    bm25 = BM25Okapi(tokenized_docs)
else:
    st.warning("BM25 ì¸ë±ìŠ¤ë¥¼ ë§Œë“¤ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„° ê²½ë¡œ ë˜ëŠ” ë¬¸ì„œ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”.")

# id -> (text, meta)
id2doc = {i: (t, m) for i, t, m in zip(filtered_ids, filtered_docs, filtered_metas)}

# ================= ê²€ìƒ‰ í•¨ìˆ˜ =================
def hybrid_retrieve(query, top_k, work_id=None):
    if not query or not query.strip():
        return []

    # 1) ë²¡í„° ê²€ìƒ‰
    emb = oa.embeddings.create(model=EMB_MODEL, input=query).data[0].embedding
    vec_res = col.query(
        query_embeddings=[emb],
        n_results=top_k * 3,
        where={"work_id": work_id} if work_id else None
    )
    vec_ids = vec_res["ids"][0] if vec_res.get("ids") else []

    # 2) BM25 ê²€ìƒ‰ (ìˆì„ ë•Œë§Œ)
    bm25_ids = []
    if bm25 is not None:
        toks = tokenize(query)
        if toks:
            scores = bm25.get_scores(toks)
            ranked = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)[:top_k*3]
            # rankedì˜ ì¸ë±ìŠ¤ëŠ” filtered_docs/tokenized_docs ê¸°ì¤€
            bm25_ids = [
                filtered_ids[i] for i, _ in ranked
                if (not work_id or (filtered_metas[i].get("work_id") == work_id))
            ]

    # 3) RRF ìœµí•©
    fused = reciprocal_rank_fusion([vec_ids, bm25_ids])

    hits = []
    for did, _ in fused:
        if did in id2doc:
            txt, meta = id2doc[did]
            hits.append((did, txt, meta))
            if len(hits) >= top_k:
                break
    return hits

# ================= í”„ë¡¬í”„íŠ¸ ìƒì„± =================
def make_prompt(query, hits, work_id=None, speak_as=None, history=[]):
    persona_block = ""
    if speak_as and work_id:
        # id2docë¥¼ ìˆœíšŒí•´ í˜ë¥´ì†Œë‚˜/ë“±ì¥ì¸ë¬¼ ì›ë¬¸ì„ ì°¾ìŒ
        persos = []
        for _id, (txt, meta) in id2doc.items():
            if meta.get("work_id") == work_id and meta.get("kind") in ["persona", "characters_raw"]:
                ch = meta.get("character", "") or ""
                if speak_as in ch:
                    persos.append(txt)
        if persos:
            persona_block = f"[ì¸ë¬¼ í˜ë¥´ì†Œë‚˜: {speak_as}]\n{persos[0]}"

    context_cards = []
    for _, txt, meta in hits:
        title = meta.get("scene_title") or meta.get("chapter_label") or meta.get("kind")
        context_cards.append(f"### {title}\n{txt}")

    system = (
        "ë‹¹ì‹ ì€ ì†Œì„¤ ì† ì¸ë¬¼ì˜ ë§íˆ¬ë¥¼ ì¬í˜„í•˜ëŠ” AIì…ë‹ˆë‹¤.\n"
        "ì»¨í…ìŠ¤íŠ¸ë¥¼ ê·¼ê±°ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.\n"
        "ë‹¹ì‹ ì´ ì†Œì„¤ ì† ë“±ì¥ì¸ë¬¼ì´ë¼ê³  ìƒê°í•˜ì„¸ìš”.\n"
        "ëŒ€í™”í•  ë•ŒëŠ” í•´ë‹¹ ì¸ë¬¼ì˜ ë§íˆ¬/ê°€ì¹˜ê´€ì„ ë°˜ì˜í•´ 1~2ë¬¸ì¥ ì´ë‚´ë¡œ ëŒ€ë‹µí•˜ì„¸ìš”.\n"
        "ë‹µí• ë•ŒëŠ” ëŒ€í™”í•˜ë“¯ì´ ìì—°ìŠ¤ëŸ½ê²Œ ì–˜ê¸°í•´"
    )
    if persona_block:
        system += "\n\n" + persona_block

    msgs = [{"role": "system", "content": system}]
    if history:
        msgs.extend(history[-6:])   # ìµœê·¼ 6í„´ë§Œ ìœ ì§€

    user = f"ì§ˆë¬¸: {query}\n\n[ì»¨í…ìŠ¤íŠ¸]\n" + "\n\n".join(context_cards[:8])
    msgs.append({"role": "user", "content": user})
    return msgs

# ================= ë‹µë³€ ìƒì„± =================
def generate(messages):
    try:
        resp = oa.responses.create(model=MODEL, input=messages)
        return getattr(resp, "output_text", "").strip()
    except Exception:
        comp = oa.chat.completions.create(model=MODEL, messages=messages)
        return comp.choices[0].message.content.strip()

# ================= Streamlit UI =================
st.set_page_config(page_title="ğŸ“š ì†Œì„¤ ìºë¦­í„° ì±—ë´‡", layout="centered")

# ğŸ‘‰ ì¹´í†¡ ìŠ¤íƒ€ì¼ CSS
st.markdown("""
<style>
html, body, .stApp { background-color: #CFE7FF !important; }
.chat-container { display: flex; flex-direction: column; padding: 20px; }
.user-message {
  background-color: #FFEB00; color: #000;
  padding: 10px 14px; border-radius: 18px 0 18px 18px;
  max-width: 70%; font-size: 15px; line-height: 1.4;
  align-self: flex-end; margin: 6px 0 6px auto;
}
.bot-message {
  background-color: #FFFFFF; color: #000;
  padding: 10px 14px; border-radius: 0 18px 18px 18px;
  max-width: 70%; font-size: 15px; line-height: 1.4;
  align-self: flex-start; margin: 6px auto 6px 0;
}
</style>
""", unsafe_allow_html=True)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "history" not in st.session_state:
    st.session_state.history = []
if "work_id" not in st.session_state:
    st.session_state.work_id = None
if "speak_as" not in st.session_state:
    st.session_state.speak_as = None

st.title("ğŸ“š ì†Œì„¤ ì† ì¸ë¬¼ê³¼ ëŒ€í™”í•˜ê¸°")

prev_work = st.session_state.get("work_id")
prev_speak = st.session_state.get("speak_as")

work_kor = st.selectbox("ì‘í’ˆ ì„ íƒ", ["ì§€êµ¬ ëì˜ ì˜¨ì‹¤", "ì¢…ì˜ ê¸°ì›", "ì†Œë…„ì´ ì˜¨ë‹¤"])
st.session_state.work_id = WORK_ID_MAP.get(work_kor)
st.session_state.speak_as = st.text_input("ì¸ë¬¼ ì„ íƒ (ì˜ˆ: ìœ ì§„, ë™í˜¸, ì•„ì˜ ë“±)", "")

# ì‘í’ˆ/ì¸ë¬¼ì´ ë°”ë€Œë©´ ëŒ€í™” ì´ˆê¸°í™”
if (prev_work and prev_work != st.session_state.work_id) or \
   (prev_speak and prev_speak != st.session_state.speak_as):
    st.session_state.history = []
    st.rerun()

# ì±„íŒ… UI
st.markdown('<div class="chat-container">', unsafe_allow_html=True)
for msg in st.session_state.history:
    if msg["role"] == "user":
        st.markdown(f'<div class="user-message">{msg["content"]}</div>', unsafe_allow_html=True)
    elif msg["role"] == "assistant":
        st.markdown(f'<div class="bot-message">{msg["content"]}</div>', unsafe_allow_html=True)
st.markdown('</div>', unsafe_allow_html=True)

# ì…ë ¥ì°½
query = st.text_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”", key="input")

if st.button("ë³´ë‚´ê¸°", type="primary") and query.strip():
    hits = hybrid_retrieve(query, TOP_K, st.session_state.work_id)
    msgs = make_prompt(query, hits,
                       work_id=st.session_state.work_id,
                       speak_as=st.session_state.speak_as,
                       history=st.session_state.history)
    ans = generate(msgs)

    # ë©”ëª¨ë¦¬ì— ê¸°ë¡
    st.session_state.history.append({"role": "user", "content": query})
    st.session_state.history.append({"role": "assistant", "content": ans})

    st.rerun()

# ì‚¬ì´ë“œë°” ì§„ë‹¨
st.sidebar.write({
    "PERSIST_DIR": PERSIST_DIR,
    "COLLECTION": COLLECTION,
    "TOP_K": TOP_K,
    "has_bm25": bm25 is not None,
    "filtered_docs": len(filtered_docs),
})



